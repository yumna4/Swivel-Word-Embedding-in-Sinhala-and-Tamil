# Swivel-Word-Embedding-in-Sinhala-and-Tamil

Swivel: Is it the Answer for Inflected Languages?

Presently word embedding is widely used in many natural language processing tasks. Local languages such as Sinhala and Tamil have been included in the natural language processing domain in the recent past. However, there exists only a few word embedding models in these local languages. Swivel is a word embedding approach that results in more accurate embedding by generating low-dimensional feature embedding from a feature co-occurrence matrix. It handles unobserved occurrences and can scale to much larger corpora than can be handled with word embedding approaches that use sampling methods. In this paper, we introduce two Swivel-based word embedding models in Sinhala and Tamil, both of which are highly inflected languages. We evaluate our models using two widely used intrinsic word embedding evaluation techniques: word similarity and word analogy. We compare the results of the proposed model with that of the only existing model of both languages, and show that Swivel outperforms the existing models. Moreover, we also introduce Sinhala and Tamil datasets of the WordSim353 and Google Analogy datasets.
